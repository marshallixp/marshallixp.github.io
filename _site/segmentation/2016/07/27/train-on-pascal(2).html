<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width initial-scale=1">

  <title>Train on Pascal(2)</title>
  <meta name="description" content="由于前一次训练中发现网络并没有收敛，经过查阅资料，我们决定采用以下调整方案：1) Reduce learning rate by a factor of 102) Normalize softmax loss (divide with the number of pixels -> normalize: true...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://yourdomain.com/segmentation/2016/07/27/train-on-pascal(2).html">
  <link rel="alternate" type="application/atom+xml" title="Deep Learning Study" href="http://yourdomain.com/feed.xml" />
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Deep Learning Study</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Train on Pascal(2)</h1>
    <p class="post-meta">Jul 27, 2016</p>
  </header>

  <article class="post-content">
    <p>由于前一次训练中发现网络并没有收敛，经过查阅资料，我们决定采用以下调整方案：<br/>
1) Reduce learning rate by a factor of 10<br/>
2) Normalize softmax loss (divide with the number of pixels -> normalize: true)<br/>
3) Try net surgery<br/>
4) mean RGB / BGR ?</p>

<p><strong>要点：</strong></p>

<p>a) Make sure you do not make the model to learn from the background (ignore_label=255)<br/>
b) IU score / pixel-wise accuracy are more important
c) Check each layer to see it's initialized with correct values</p>

<p><strong>Fcn32s Training</strong></p>

<p>根据调整方案1），目前采用fixed的学习率，取值1e-9。根据2），将softmaxloss层的normalize设置为true。根据修改后的参数模型，经过4000次迭代以后，我们发现loss是在不断减小，说明模型可以收敛，但是主要的指标参数依然没有改变，需要等待进一步的结果。</p>

<pre><code>&gt;&gt;&gt; 2016-07-27 15:58:21.062000 Iteration 4000 loss 3.04448500594  
&gt;&gt;&gt; 2016-07-27 15:58:21.062184 Iteration 4000 overall accuracy 0.733180213097  
&gt;&gt;&gt; 2016-07-27 15:58:21.062357 Iteration 4000 mean accuracy 0.047619047619  
&gt;&gt;&gt; 2016-07-27 15:58:21.062504 Iteration 4000 mean IU 0.0349133434808  
&gt;&gt;&gt; 2016-07-27 15:58:21.062591 Iteration 4000 fwavacc 0.537553224877  
</code></pre>

<p>当从32000次迭代结果恢复以后，我们发现网络的损失是以很小的数值下降，约为3e-5的减小值。但是，主要指标依然没有变化。</p>

<pre><code>&gt;&gt;&gt; 2016-07-28 12:56:20.636460 Begin seg tests
&gt;&gt;&gt; 2016-07-28 12:59:02.790584 Iteration 48000 loss 3.04407284242
&gt;&gt;&gt; 2016-07-28 12:59:02.790747 Iteration 48000 overall accuracy 0.733180213097
&gt;&gt;&gt; 2016-07-28 12:59:02.790932 Iteration 48000 mean accuracy 0.047619047619
&gt;&gt;&gt; 2016-07-28 12:59:02.791069 Iteration 48000 mean IU 0.0349133434808
&gt;&gt;&gt; 2016-07-28 12:59:02.791161 Iteration 48000 fwavacc 0.537553224877
&gt;&gt;&gt; 2016-07-28 13:21:27.470571 Begin seg tests
&gt;&gt;&gt; 2016-07-28 13:24:08.706496 Iteration 52000 loss 3.04403515862
&gt;&gt;&gt; 2016-07-28 13:24:08.706656 Iteration 52000 overall accuracy 0.733180213097
&gt;&gt;&gt; 2016-07-28 13:24:08.706831 Iteration 52000 mean accuracy 0.047619047619
&gt;&gt;&gt; 2016-07-28 13:24:08.706970 Iteration 52000 mean IU 0.0349133434808
&gt;&gt;&gt; 2016-07-28 13:24:08.707063 Iteration 52000 fwavacc 0.537553224877  
</code></pre>

<p>对网络层进行检查，我们很遗憾地发现fc6_conv, fc7_conv, score_fr层，并没有被初始化，也没有训练更新，这是目前网络无法收敛的原因。</p>

<p>根据上述分析，采取如下调整：
1) vgg16相关的层冻结，对fc6_conv, fc7_conv, score_fr, upscore激活学习参数,并且初始化，如下：<br/>
param {
    lr_mult: 1
    decay_mult: 1
}<br/>
param {
    lr_mult: 2
    decay_mult: 0
}<br/>
convolution_param {
    num_output: 4096
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
}</p>

<p>2）采用fixed的学习率1e-10，monentum为0.99。在经过200次迭代之后，我们观察相关层的变化情况。然而，发现除了初始化成功外，依然没有进行训练，即相关层的权重几乎没有改变（1000次迭代，除了score_fr层有微小改变）。</p>

<p>考虑到可能是学习率设置太小，现将学习率改为1e-6, momentum为0.9，再次进行训练。发现果然如预期，除了固定权重层（vgg对应）以外，其余各层权重均开始改变。设想训练参数及网络模型有效，需要进行再次训练验证。同时，根据文章中的参数设置，我们重新设置训练主要参数如下：<br/>
lr_policy: "fixed"<br/>
base_lr: 1e-4<br/>
momentum: 0.9<br/>
weight_decay: 0.0005<br/>
于是，结果如下：</p>

<pre><code>&gt;&gt;&gt; 2016-07-28 15:37:43.526054 Begin seg tests
&gt;&gt;&gt; 2016-07-28 15:40:30.566007 Iteration 4000 loss 0.778730130796
&gt;&gt;&gt; 2016-07-28 15:40:30.566297 Iteration 4000 overall accuracy 0.833765982794
&gt;&gt;&gt; 2016-07-28 15:40:30.566349 Iteration 4000 mean accuracy 0.654612851421
&gt;&gt;&gt; 2016-07-28 15:40:30.566486 Iteration 4000 mean IU 0.450926431181
&gt;&gt;&gt; 2016-07-28 15:40:30.566580 Iteration 4000 fwavacc 0.742218942131
&gt;&gt;&gt; 2016-07-28 15:53:21.709648 Begin seg tests
&gt;&gt;&gt; 2016-07-28 15:56:08.182363 Iteration 8000 loss 0.865841977427
&gt;&gt;&gt; 2016-07-28 15:56:08.182477 Iteration 8000 overall accuracy 0.804960917114
&gt;&gt;&gt; 2016-07-28 15:56:08.182606 Iteration 8000 mean accuracy 0.723366231692
&gt;&gt;&gt; 2016-07-28 15:56:08.182786 Iteration 8000 mean IU 0.450797861465
&gt;&gt;&gt; 2016-07-28 15:56:08.182920 Iteration 8000 fwavacc 0.715707726933
&gt;&gt;&gt; 2016-07-28 16:08:59.654618 Begin seg tests
&gt;&gt;&gt; 2016-07-28 16:11:45.822145 Iteration 12000 loss 0.72744285975
&gt;&gt;&gt; 2016-07-28 16:11:45.822326 Iteration 12000 overall accuracy 0.829063382524
&gt;&gt;&gt; 2016-07-28 16:11:45.822399 Iteration 12000 mean accuracy 0.728383144052
&gt;&gt;&gt; 2016-07-28 16:11:45.822697 Iteration 12000 mean IU 0.470663378357
&gt;&gt;&gt; 2016-07-28 16:11:45.822794 Iteration 12000 fwavacc 0.738958147784
&gt;&gt;&gt; 2016-07-28 16:24:43.682142 Begin seg tests
&gt;&gt;&gt; 2016-07-28 16:27:32.582254 Iteration 16000 loss 0.671988376899
&gt;&gt;&gt; 2016-07-28 16:27:32.582414 Iteration 16000 overall accuracy 0.834907019939
&gt;&gt;&gt; 2016-07-28 16:27:32.582577 Iteration 16000 mean accuracy 0.730903583042
&gt;&gt;&gt; 2016-07-28 16:27:32.582713 Iteration 16000 mean IU 0.474480346083
&gt;&gt;&gt; 2016-07-28 16:27:32.582808 Iteration 16000 fwavacc 0.746916759749
</code></pre>

<p>可见，最终主要指标都出现大幅提升，说明训练有效！</p>

<p><strong>SegNet Training</strong></p>

<p>根据调整方案1)，采用fixed的学习率1e-10，monentum为0.99。根据2），将softmaxloss层的normalize设置为true。<br/>
同样，经过4000次迭代以后，我们发现修改后的参数模型，loss可以不断减小，即表示可以收敛，而且收敛速度快于Fcn32s网络，但是最主要的指标参数也依然没有出现变化，需等待进一步训练结果。</p>

<pre><code>&gt;&gt;&gt; 2016-07-27 16:36:01.449254 Iteration 4000 loss 3.04294351251  
&gt;&gt;&gt; 2016-07-27 16:36:01.449524 Iteration 4000 overall accuracy 0.733180213097  
&gt;&gt;&gt; 2016-07-27 16:36:01.449573 Iteration 4000 mean accuracy 0.047619047619  
&gt;&gt;&gt; 2016-07-27 16:36:01.449706 Iteration 4000 mean IU 0.0349133434808  
&gt;&gt;&gt; 2016-07-27 16:36:01.449798 Iteration 4000 fwavacc 0.537553224877  
</code></pre>

<p>由于之前的Fcn32s网络实现了有效训练，我们将相关调整也作用于SegNet网络上（选用相同的训练参数），测试结果。</p>

<p>正如预期，SegNet也可以较为快速的收敛，并且相应层参数均发生改变。最后，我们对整个网络进行训练测试，其结果如下：</p>

<pre><code>&gt;&gt;&gt; 2016-07-28 16:47:44.193496 Iteration 4000 loss 1.34012441367
&gt;&gt;&gt; 2016-07-28 16:47:44.193629 Iteration 4000 overall accuracy 0.733180213097
&gt;&gt;&gt; 2016-07-28 16:47:44.193757 Iteration 4000 mean accuracy 0.047619047619
&gt;&gt;&gt; 2016-07-28 16:47:44.194040 Iteration 4000 mean IU 0.0349133434808
&gt;&gt;&gt; 2016-07-28 16:47:44.194139 Iteration 4000 fwavacc 0.537553224877
</code></pre>

<p>结果发现主要参数指标并没有改变，对相关层进行检查，发现decoder层(conv1_1_D)权重并未有效地学习。然而，对conv1_1_D的权重进行初始化，依然不能使得网络有效地收敛，如下：</p>

<pre><code>&gt;&gt;&gt; 2016-07-28 19:19:44.126946 Iteration 24254 loss 1.30797043911
&gt;&gt;&gt; 2016-07-28 19:19:44.127122 Iteration 24254 overall accuracy 0.733161065479
&gt;&gt;&gt; 2016-07-28 19:19:44.127300 Iteration 24254 mean accuracy 0.0476180853631
&gt;&gt;&gt; 2016-07-28 19:19:44.127435 Iteration 24254 mean IU 0.0349127414014
&gt;&gt;&gt; 2016-07-28 19:19:44.127536 Iteration 24254 fwavacc 0.537539625039
</code></pre>

<p>接下来，调整网络模型，设置convolution层的参数为type='gaussian', std=0.01。我们发现依然很难收敛。于是，为了验证添加多个crop层是否正确，通过之前已经成功在CamVid训练集上测试的SegNet网络，再使用更改后的Caffe在新的网络上（添加多个crop层）进行测试。我们发现同样在CamVid上训练，SegNet(1e-3)似乎能有比Fcn32s(1e-4)更快的收敛速度。</p>

<p><strong>小结</strong></p>

<p>对于fcn32s网络来说，由于一开始并没有弄清楚各个训练参数和网络模型参数的意思，始终将vgg16对应层进入训练，相反最后的score_fr和upscore层并没有进行训练;除此以外，除vgg16对应层外的如fc6_conv等并没有有效初始化，无法实现参数更新。而且，当训练参数如base_lr设置太小（如1e-10）也会导致训练收敛速度过慢。当然，normalize softmax loss也是必须的。</p>

<p>PS:　对于fcn32s相较于SegNet来说，decoder部分层数少，所有训练过程速度要快于后者。</p>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Deep Learning Study</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>Deep Learning Study</li>
          <li><a href="mailto:marshallixp@hotmail.com">marshallixp@hotmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/marshallixp">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">marshallixp</span>
            </a>
          </li>
          

          
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text">Explain the state-of-art research and Track our latest experiments.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
