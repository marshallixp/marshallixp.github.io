<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning Study</title>
    <description>Explain the state-of-art research and Track our latest experiments.
</description>
    <link>http://yourdomain.com/</link>
    <atom:link href="http://yourdomain.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 28 Jul 2016 15:39:25 +0800</pubDate>
    <lastBuildDate>Thu, 28 Jul 2016 15:39:25 +0800</lastBuildDate>
    <generator>Jekyll v2.5.0</generator>
    
      <item>
        <title>Train on Pascal(2)</title>
        <description>&lt;p&gt;由于前一次训练中发现网络并没有收敛，经过查阅资料，我们决定采用以下调整方案：&lt;br/&gt;
1) Reduce learning rate by a factor of 10&lt;br/&gt;
2) Normalize softmax loss (divide with the number of pixels -&gt; normalize: true)&lt;br/&gt;
3) Try net surgery&lt;br/&gt;
4) mean RGB / BGR ?&lt;/p&gt;

&lt;p&gt;要点：
a) Make sure you do not make the model to learn from the background (ignore_label=255)&lt;br/&gt;
b) IU score / pixel-wise accuracy are more important
c) Check each layer to see it's initialized with correct values&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SegNet Training&lt;/strong&gt;
根据调整方案1)，采用fixed的学习率1e-10，monentum为0.99。根据2），将softmaxloss层的normalize设置为true。&lt;br/&gt;
同样，经过4000次迭代以后，我们发现修改后的参数模型，loss可以不断减小，即表示可以收敛，而且收敛速度快于Fcn32s网络，但是最主要的指标参数也依然没有出现变化，需等待进一步训练结果。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; 2016-07-27 16:36:01.449254 Iteration 4000 loss 3.04294351251  
&amp;gt;&amp;gt;&amp;gt; 2016-07-27 16:36:01.449524 Iteration 4000 overall accuracy 0.733180213097  
&amp;gt;&amp;gt;&amp;gt; 2016-07-27 16:36:01.449573 Iteration 4000 mean accuracy 0.047619047619  
&amp;gt;&amp;gt;&amp;gt; 2016-07-27 16:36:01.449706 Iteration 4000 mean IU 0.0349133434808  
&amp;gt;&amp;gt;&amp;gt; 2016-07-27 16:36:01.449798 Iteration 4000 fwavacc 0.537553224877  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Fcn32s Training&lt;/strong&gt;
根据调整方案1），目前采用fixed的学习率，取值1e-9。根据2），将softmaxloss层的normalize设置为true。&lt;br/&gt;
根据修改后的参数模型，经过4000次迭代以后，我们发现loss是在不断减小，说明模型可以收敛，但是主要的指标参数依然没有改变，需要等待进一步的结果。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; 2016-07-27 15:58:21.062000 Iteration 4000 loss 3.04448500594  
&amp;gt;&amp;gt;&amp;gt; 2016-07-27 15:58:21.062184 Iteration 4000 overall accuracy 0.733180213097  
&amp;gt;&amp;gt;&amp;gt; 2016-07-27 15:58:21.062357 Iteration 4000 mean accuracy 0.047619047619  
&amp;gt;&amp;gt;&amp;gt; 2016-07-27 15:58:21.062504 Iteration 4000 mean IU 0.0349133434808  
&amp;gt;&amp;gt;&amp;gt; 2016-07-27 15:58:21.062591 Iteration 4000 fwavacc 0.537553224877  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当从32000次迭代结果恢复以后，我们发现网络的损失是以很小的数值下降，约为3e-5的减小值。但是，主要指标依然没有变化。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; 2016-07-28 12:56:20.636460 Begin seg tests
&amp;gt;&amp;gt;&amp;gt; 2016-07-28 12:59:02.790584 Iteration 48000 loss 3.04407284242
&amp;gt;&amp;gt;&amp;gt; 2016-07-28 12:59:02.790747 Iteration 48000 overall accuracy 0.733180213097
&amp;gt;&amp;gt;&amp;gt; 2016-07-28 12:59:02.790932 Iteration 48000 mean accuracy 0.047619047619
&amp;gt;&amp;gt;&amp;gt; 2016-07-28 12:59:02.791069 Iteration 48000 mean IU 0.0349133434808
&amp;gt;&amp;gt;&amp;gt; 2016-07-28 12:59:02.791161 Iteration 48000 fwavacc 0.537553224877
&amp;gt;&amp;gt;&amp;gt; 2016-07-28 13:21:27.470571 Begin seg tests
&amp;gt;&amp;gt;&amp;gt; 2016-07-28 13:24:08.706496 Iteration 52000 loss 3.04403515862
&amp;gt;&amp;gt;&amp;gt; 2016-07-28 13:24:08.706656 Iteration 52000 overall accuracy 0.733180213097
&amp;gt;&amp;gt;&amp;gt; 2016-07-28 13:24:08.706831 Iteration 52000 mean accuracy 0.047619047619
&amp;gt;&amp;gt;&amp;gt; 2016-07-28 13:24:08.706970 Iteration 52000 mean IU 0.0349133434808
&amp;gt;&amp;gt;&amp;gt; 2016-07-28 13:24:08.707063 Iteration 52000 fwavacc 0.537553224877  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对网络层进行检查，我们很遗憾地发现fc6_conv, fc7_conv, score_fr层，并没有被初始化，也没有训练更新，这是目前网络无法收敛的原因。&lt;/p&gt;

&lt;p&gt;根据上述分析，采取如下调整：
1) vgg16相关的层冻结，对fc6_conv, fc7_conv, score_fr, upscore激活学习参数,并且初始化，如下：&lt;br/&gt;
param {
    lr_mult: 1
    decay_mult: 1
}&lt;br/&gt;
param {
    lr_mult: 2
    decay_mult: 0
}&lt;br/&gt;
convolution_param {
    num_output: 4096
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
      value: 0.1
    }
}&lt;/p&gt;

&lt;p&gt;2）采用fixed的学习率1e-10，monentum为0.99。在经过200次迭代之后，我们观察相关层的变化情况。然而，发现除了初始化成功外，依然没有进行训练，即相关层的权重几乎没有改变（1000次迭代，除了score_fr层有微小改变）。&lt;/p&gt;

&lt;p&gt;考虑到可能是学习率设置太小，现将学习率改为1e-6, momentum为0.9，再次进行训练。发现果然如预期，除了固定权重层（vgg对应）以外，其余各层权重均开始改变。设想训练参数及网络模型有效，需要进行再次训练验证。同时，根据文章中的参数设置，我们重新设置训练主要参数如下：&lt;br/&gt;
lr_policy: &quot;fixed&quot;&lt;br/&gt;
base_lr: 1e-4&lt;br/&gt;
momentum: 0.9&lt;br/&gt;
weight_decay: 0.0005&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;小结&lt;/strong&gt;
由于一开始并没有弄清楚各个训练参数和网络模型参数的意思，始终将vgg16对应层进入训练，相反最后的score_fr和upscore层并没有进行训练;除此以外，除vgg16对应层外的如fc6_conv等并没有有效初始化，无法实现参数更新。而且，当训练参数如base_lr设置太小（如1e-10）也会导致训练收敛速度过慢。当然，normalize softmax loss也是必须的。&lt;/p&gt;
</description>
        <pubDate>Wed, 27 Jul 2016 23:09:00 +0800</pubDate>
        <link>http://yourdomain.com/segmentation/2016/07/27/train-on-pascal(2).html</link>
        <guid isPermaLink="true">http://yourdomain.com/segmentation/2016/07/27/train-on-pascal(2).html</guid>
        
        
        <category>segmentation</category>
        
      </item>
    
      <item>
        <title>Train on Pascal(1)</title>
        <description>&lt;p&gt;&lt;strong&gt;SegNet Training&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Step1. 不训练（没有反向过程），直接测试&lt;/em&gt;&lt;br/&gt;
1）不用pretrained vgg16模型&lt;br/&gt;
2）使用pretrained vgg16模型&lt;br/&gt;
其中，1）结果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;2016-07-25 15:29:18.270980 Iteration 0 loss 87.3365001902  
&amp;gt;&amp;gt;&amp;gt;2016-07-25 15:29:18.271152 Iteration 0 overall accuracy 0.733180213097  
&amp;gt;&amp;gt;&amp;gt;2016-07-25 15:29:18.271215 Iteration 0 mean accuracy 0.047619047619  
&amp;gt;&amp;gt;&amp;gt;2016-07-25 15:29:18.271508 Iteration 0 mean IU 0.0349133434808  
&amp;gt;&amp;gt;&amp;gt;2016-07-25 15:29:18.271601 Iteration 0 fwavacc 0.537553224877  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;相应，2）结果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;2016-07-25 15:35:54.661197 Iteration 0 loss 87.3365001902  
&amp;gt;&amp;gt;&amp;gt;2016-07-25 15:35:54.661321 Iteration 0 overall accuracy 0.733180213097  
&amp;gt;&amp;gt;&amp;gt;2016-07-25 15:35:54.661488 Iteration 0 mean accuracy 0.047619047619  
&amp;gt;&amp;gt;&amp;gt;2016-07-25 15:35:54.661621 Iteration 0 mean IU 0.0349133434808  
&amp;gt;&amp;gt;&amp;gt;2016-07-25 15:35:54.661709 Iteration 0 fwavacc 0.537553224877  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;很明显，无论是否采用vgg16对没有经过训练网络的结果没有影响。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Step2. 训练，再测试&lt;/em&gt;&lt;br/&gt;
solver.protoxt中参数根据文章中的设置如下：&lt;br/&gt;
train_net: &quot;train.prototxt&quot;&lt;br/&gt;
test_net: &quot;val.prototxt&quot;&lt;br/&gt;
test_initialization: false&lt;br/&gt;
test_iter: 736&lt;br/&gt;
test_interval: 999999999&lt;br/&gt;
display: 20&lt;br/&gt;
average_loss: 20&lt;br/&gt;
lr_policy: &quot;fixed&quot;&lt;br/&gt;
base_lr: 1e-4&lt;br/&gt;
momentum: 0.9&lt;br/&gt;
iter_size: 1&lt;br/&gt;
max_iter: 100000&lt;br/&gt;
weight_decay: 0.0005&lt;br/&gt;
snapshot: 4000&lt;br/&gt;
snapshot_prefix: &quot;snapshot/segnet&quot;&lt;br/&gt;
solver_mode: GPU&lt;/p&gt;

&lt;p&gt;1）使用pretrained vgg16模型，训练4000次，结果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;2016-07-26 12:23:04.827459 Iteration 4000 loss 1.31175746871  
&amp;gt;&amp;gt;&amp;gt;2016-07-26 12:23:04.827577 Iteration 4000 overall accuracy 0.733180213097  
&amp;gt;&amp;gt;&amp;gt;2016-07-26 12:23:04.827639 Iteration 4000 mean accuracy 0.047619047619  
&amp;gt;&amp;gt;&amp;gt;2016-07-26 12:23:04.827855 Iteration 4000 mean IU 0.0349133434808  
&amp;gt;&amp;gt;&amp;gt;2016-07-26 12:23:04.828067 Iteration 4000 fwavacc 0.537553224877  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在经过4000次迭代，大约2.5倍样本集后，loss在缓慢下降，但是其他测量结果没有改变。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;2016-07-26 17:29:48.209541 Iteration 48000 loss 1.56178289191  
&amp;gt;&amp;gt;&amp;gt;2016-07-26 17:29:48.209808 Iteration 48000 overall accuracy 0.733170522908  
&amp;gt;&amp;gt;&amp;gt;2016-07-26 17:29:48.209855 Iteration 48000 mean accuracy 0.0476187852268  
&amp;gt;&amp;gt;&amp;gt;2016-07-26 17:29:48.209980 Iteration 48000 mean IU 0.0349132915075  
&amp;gt;&amp;gt;&amp;gt;2016-07-26 17:29:48.210064 Iteration 48000 fwavacc 0.537546775601  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在经过48000次迭代以后，loss并没有收敛，训练失败，待续。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fcn32s Training&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;首先，我们下载了训练好的模型fcn32s-heavy-model，发现经过finetune以后，结果会更好。&lt;br/&gt;
&lt;em&gt;Fcn32s fine-tune with pretrained heavy model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;2016-07-11 17:11:40.982484 Iteration 4000 loss 40536.5665925  
&amp;gt;&amp;gt;&amp;gt;2016-07-11 17:11:40.982576 Iteration 4000 overall accuracy 0.924534222757  
&amp;gt;&amp;gt;&amp;gt;2016-07-11 17:11:40.982608 Iteration 4000 mean accuracy 0.789070139032   
&amp;gt;&amp;gt;&amp;gt;2016-07-11 17:11:40.982729 Iteration 4000 mean IU 0.68489801572  
&amp;gt;&amp;gt;&amp;gt;2016-07-11 17:11:40.982804 Iteration 4000 fwavacc 0.867180920774  

&amp;gt;&amp;gt;&amp;gt;2016-07-11 17:34:39.726955 Iteration 8000 loss 40918.7736482  
&amp;gt;&amp;gt;&amp;gt;2016-07-11 17:34:39.727035 Iteration 8000 overall accuracy 0.928679202249  
&amp;gt;&amp;gt;&amp;gt;2016-07-11 17:34:39.727064 Iteration 8000 mean accuracy 0.812085149284  
&amp;gt;&amp;gt;&amp;gt;2016-07-11 17:34:39.727181 Iteration 8000 mean IU 0.698756970877  
&amp;gt;&amp;gt;&amp;gt;2016-07-11 17:34:39.727255 Iteration 8000 fwavacc 0.873438191274  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Fcn32s + pretrained heavy model without training&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;2016-07-11 17:40:52.292152 Iteration 0 loss 45737.5351437  
&amp;gt;&amp;gt;&amp;gt;2016-07-11 17:40:52.292247 Iteration 0 overall accuracy 0.926090782406  
&amp;gt;&amp;gt;&amp;gt;2016-07-11 17:40:52.292278 Iteration 0 mean accuracy 0.817075271365  
&amp;gt;&amp;gt;&amp;gt;2016-07-11 17:40:52.292399 Iteration 0 mean IU 0.695001155057  
&amp;gt;&amp;gt;&amp;gt;2016-07-11 17:40:52.292475 Iteration 0 fwavacc 0.869301769793  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其次，我们利用pretrained VGG16进行训练。 目前得到的结果显示，loss会降低，但是其他测量结果并没有发生改变，如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;2016-07-26 12:48:33.904036 Iteration 8000 loss 3916726.69319  
&amp;gt;&amp;gt;&amp;gt;2016-07-26 12:48:33.904141 Iteration 8000 overall accuracy 0.733180213097  
&amp;gt;&amp;gt;&amp;gt;2016-07-26 12:48:33.904256 Iteration 8000 mean accuracy 0.047619047619  
&amp;gt;&amp;gt;&amp;gt;2016-07-26 12:48:33.904529 Iteration 8000 mean IU 0.0349133434808  
&amp;gt;&amp;gt;&amp;gt;2016-07-26 12:48:33.904619 Iteration 8000 fwavacc 0.53755322487  

&amp;gt;&amp;gt;&amp;gt;2016-07-26 13:39:00.477857 Iteration 16000 loss 3646240.50901  
&amp;gt;&amp;gt;&amp;gt;2016-07-26 13:39:00.477998 Iteration 16000 overall accuracy 0.733180213097
&amp;gt;&amp;gt;&amp;gt;2016-07-26 13:39:00.478118 Iteration 16000 mean accuracy 0.047619047619  
&amp;gt;&amp;gt;&amp;gt;2016-07-26 13:39:00.478241 Iteration 16000 mean IU 0.0349133434808  
&amp;gt;&amp;gt;&amp;gt;2016-07-26 13:39:00.478323 Iteration 16000 fwavacc 0.537553224877  

&amp;gt;&amp;gt;&amp;gt;2016-07-26 17:03:43.480082 Iteration 48000 loss 3076769.60242  
&amp;gt;&amp;gt;&amp;gt;2016-07-26 17:03:43.480186 Iteration 48000 overall accuracy 0.733180213097
&amp;gt;&amp;gt;&amp;gt;2016-07-26 17:03:43.480288 Iteration 48000 mean accuracy 0.047619047619  
&amp;gt;&amp;gt;&amp;gt;2016-07-26 17:03:43.480508 Iteration 48000 mean IU 0.0349133434808  
&amp;gt;&amp;gt;&amp;gt;2016-07-26 17:03:43.480592 Iteration 48000 fwavacc 0.537553224877  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;经过近50000次的训练，发现其中loss并没有收敛，而且另外4个指标也完全没有任何改变，和未经过训练的结果相同。可见，这里存在什么问题还没有弄清楚，待续。&lt;/p&gt;
</description>
        <pubDate>Wed, 27 Jul 2016 01:09:59 +0800</pubDate>
        <link>http://yourdomain.com/segmentation/2016/07/27/train-on-pascal(1).html</link>
        <guid isPermaLink="true">http://yourdomain.com/segmentation/2016/07/27/train-on-pascal(1).html</guid>
        
        
        <category>segmentation</category>
        
      </item>
    
      <item>
        <title>Train on CamVid</title>
        <description>&lt;p&gt;SegNet on CamVid&lt;/p&gt;

&lt;p&gt;-- SegNet + pretrained VGG16
-&gt; 10000 iteration (currently)
   Global acc = 0.85041 Class average acc = 0.48181 Mean Int over Union = 0.414&lt;/p&gt;
</description>
        <pubDate>Wed, 27 Jul 2016 01:09:59 +0800</pubDate>
        <link>http://yourdomain.com/segmentation/2016/07/27/train-on-camvid.html</link>
        <guid isPermaLink="true">http://yourdomain.com/segmentation/2016/07/27/train-on-camvid.html</guid>
        
        
        <category>segmentation</category>
        
      </item>
    
      <item>
        <title>solver文件各参数说明文档</title>
        <description>&lt;pre&gt;&lt;code&gt;train_net/test_net ： 网络定义文件的路径
test_iter : 如果数据库有1000个样本，batch_size为10，则需要100次iterations. 因而定义了test_iter，则表示测试阶段每次数据量batch_size
test_interval : 训练阶段，定义迭代的次数进行测试，如500次迭代一次测试
base_lr ： 初始学习速率
- fixed : 保持base_lr不变
- step ： 需设置stepsize, 返回base_lr * gamma ^ (floor(iter / stepsize))
- inv : 需设置power，返回base_lr * (1 + gamma*iter)^(-power)
- poly : 进行多项式误差，返回base_lr*(1-iter/max_iter)^power
- sigmoid ： 进行sigmoid衰减，返回base_lr*(1/(1+exp(-gamma*(iter-stepsize))))
momentum : 含该参数的sgd
weight_decay : 权重衰减
lr_policy : 学习参数的衰减方式
gamma, power
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;
solver = caffe.SGDSolver(&quot;solver.prototxt&quot;)&lt;br/&gt;
如果进行fine-tune，则需要调用&lt;br/&gt;
solver.net.copy_from('xxx.caffemodel')
可以继续未完成的训练，通过调用&lt;br/&gt;
solver.restore('xxx.solverstate')&lt;/p&gt;

&lt;p&gt;在fine-tune过程中，参数设置如下可以防止训练过程中对载入的参数数值的更改。
param {&lt;br/&gt;
    lr_mult: 1&lt;br/&gt;
    decay_mult: 1&lt;br/&gt;
}&lt;/p&gt;

&lt;p&gt;example/net_surgery.ipynb是有效地介绍如何使用surgery的内容。&lt;/p&gt;
</description>
        <pubDate>Wed, 27 Jul 2016 00:44:59 +0800</pubDate>
        <link>http://yourdomain.com/segmentation/2016/07/27/train-overall.html</link>
        <guid isPermaLink="true">http://yourdomain.com/segmentation/2016/07/27/train-overall.html</guid>
        
        
        <category>segmentation</category>
        
      </item>
    
  </channel>
</rss>
