<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning Study</title>
    <description>Explain the state-of-art research and Track our latest experiments.
</description>
    <link>http://yourdomain.com/</link>
    <atom:link href="http://yourdomain.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 27 Jul 2016 14:37:14 +0800</pubDate>
    <lastBuildDate>Wed, 27 Jul 2016 14:37:14 +0800</lastBuildDate>
    <generator>Jekyll v2.5.0</generator>
    
      <item>
        <title>Train on Pascal</title>
        <description>&lt;p&gt;&lt;strong&gt;SegNet Training&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Step1. 不训练（没有反向过程），直接测试&lt;/em&gt;&lt;br/&gt;
1）不用pretrained模型&lt;br/&gt;
2）使用pretrained模型&lt;br/&gt;
其中，1）结果如下：&lt;br/&gt;
2016-07-25 15:29:18.270980 Iteration 0 loss 87.3365001902&lt;br/&gt;
2016-07-25 15:29:18.271152 Iteration 0 overall accuracy 0.733180213097&lt;br/&gt;
2016-07-25 15:29:18.271215 Iteration 0 mean accuracy 0.047619047619&lt;br/&gt;
2016-07-25 15:29:18.271508 Iteration 0 mean IU 0.0349133434808&lt;br/&gt;
2016-07-25 15:29:18.271601 Iteration 0 fwavacc 0.537553224877&lt;br/&gt;
相应，2）结果如下：&lt;br/&gt;
2016-07-25 15:35:54.661197 Iteration 0 loss 87.3365001902&lt;br/&gt;
2016-07-25 15:35:54.661321 Iteration 0 overall accuracy 0.733180213097&lt;br/&gt;
2016-07-25 15:35:54.661488 Iteration 0 mean accuracy 0.047619047619&lt;br/&gt;
2016-07-25 15:35:54.661621 Iteration 0 mean IU 0.0349133434808&lt;br/&gt;
2016-07-25 15:35:54.661709 Iteration 0 fwavacc 0.537553224877&lt;br/&gt;
很明显，无论是否采用vgg16对没有经过训练网络的结果没有影响。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Step2. 训练，再测试&lt;/em&gt;&lt;br/&gt;
solver.protoxt中参数根据文章中的设置如下：&lt;br/&gt;
train_net: &quot;train.prototxt&quot;&lt;br/&gt;
test_net: &quot;val.prototxt&quot;&lt;br/&gt;
test_initialization: false&lt;br/&gt;
test_iter: 736&lt;br/&gt;
test_interval: 999999999&lt;br/&gt;
display: 20&lt;br/&gt;
average_loss: 20&lt;br/&gt;
lr_policy: &quot;fixed&quot;&lt;br/&gt;
base_lr: 1e-4&lt;br/&gt;
momentum: 0.9&lt;br/&gt;
iter_size: 1&lt;br/&gt;
max_iter: 100000&lt;br/&gt;
weight_decay: 0.0005&lt;br/&gt;
snapshot: 4000&lt;br/&gt;
snapshot_prefix: &quot;snapshot/segnet&quot;&lt;br/&gt;
solver_mode: GPU&lt;/p&gt;

&lt;p&gt;1）使用pretrained VGG16模型，每次训练步长为4000，训练结果如下：&lt;br/&gt;
2016-07-26 12:23:04.827459 Iteration 4000 loss 1.31175746871&lt;br/&gt;
2016-07-26 12:23:04.827577 Iteration 4000 overall accuracy 0.733180213097&lt;br/&gt;
2016-07-26 12:23:04.827639 Iteration 4000 mean accuracy 0.047619047619&lt;br/&gt;
2016-07-26 12:23:04.827855 Iteration 4000 mean IU 0.0349133434808&lt;br/&gt;
2016-07-26 12:23:04.828067 Iteration 4000 fwavacc 0.537553224877&lt;br/&gt;
在经过4000次迭代，大约2.5倍样本集后，发现loss在缓慢下降，但是其他测量结果没有改变。&lt;/p&gt;

&lt;p&gt;2016-07-26 17:29:48.209541 Iteration 48000 loss 1.56178289191&lt;br/&gt;
2016-07-26 17:29:48.209808 Iteration 48000 overall accuracy 0.733170522908&lt;br/&gt;
2016-07-26 17:29:48.209855 Iteration 48000 mean accuracy 0.0476187852268&lt;br/&gt;
2016-07-26 17:29:48.209980 Iteration 48000 mean IU 0.0349132915075&lt;br/&gt;
2016-07-26 17:29:48.210064 Iteration 48000 fwavacc 0.537546775601&lt;br/&gt;
在经过48000次迭代以后，loss并没有收敛，训练失败，待续。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fcn32s Training&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;首先，我们下载了训练好的模型fcn32s-heavy-model，发现经过finetune以后，结果会更好。&lt;br/&gt;
&lt;em&gt;Fcn32s fine-tune with pretrained heavy model&lt;/em&gt;&lt;br/&gt;
2016-07-11 17:09:09.316152 Begin seg tests&lt;br/&gt;
2016-07-11 17:11:40.982484 Iteration 4000 loss 40536.5665925&lt;br/&gt;
2016-07-11 17:11:40.982576 Iteration 4000 overall accuracy 0.924534222757&lt;br/&gt;
2016-07-11 17:11:40.982608 Iteration 4000 mean accuracy 0.789070139032 &lt;br/&gt;
2016-07-11 17:11:40.982729 Iteration 4000 mean IU 0.68489801572&lt;br/&gt;
2016-07-11 17:11:40.982804 Iteration 4000 fwavacc 0.867180920774&lt;/p&gt;

&lt;p&gt;2016-07-11 17:32:13.296804 Begin seg tests&lt;br/&gt;
2016-07-11 17:34:39.726955 Iteration 8000 loss 40918.7736482&lt;br/&gt;
2016-07-11 17:34:39.727035 Iteration 8000 overall accuracy 0.928679202249&lt;br/&gt;
2016-07-11 17:34:39.727064 Iteration 8000 mean accuracy 0.812085149284&lt;br/&gt;
2016-07-11 17:34:39.727181 Iteration 8000 mean IU 0.698756970877&lt;br/&gt;
2016-07-11 17:34:39.727255 Iteration 8000 fwavacc 0.873438191274&lt;/p&gt;

&lt;p&gt;Fcn32s + pretrained heavy model without training&lt;br/&gt;
2016-07-11 17:38:19.174321 Begin seg tests&lt;br/&gt;
2016-07-11 17:40:52.292152 Iteration 0 loss 45737.5351437&lt;br/&gt;
2016-07-11 17:40:52.292247 Iteration 0 overall accuracy 0.926090782406&lt;br/&gt;
2016-07-11 17:40:52.292278 Iteration 0 mean accuracy 0.817075271365&lt;br/&gt;
2016-07-11 17:40:52.292399 Iteration 0 mean IU 0.695001155057&lt;br/&gt;
2016-07-11 17:40:52.292475 Iteration 0 fwavacc 0.869301769793&lt;/p&gt;

&lt;p&gt;其次，我们利用pretrained VGG16进行训练.目前得到的结果显示，loss会降低，但是其他测量结果并没有发生改变，如下：&lt;br/&gt;
2016-07-26 12:48:33.904036 Iteration 8000 loss 3916726.69319&lt;br/&gt;
2016-07-26 12:48:33.904141 Iteration 8000 overall accuracy 0.733180213097&lt;br/&gt;
2016-07-26 12:48:33.904256 Iteration 8000 mean accuracy 0.047619047619&lt;br/&gt;
2016-07-26 12:48:33.904529 Iteration 8000 mean IU 0.0349133434808&lt;br/&gt;
2016-07-26 12:48:33.904619 Iteration 8000 fwavacc 0.53755322487&lt;/p&gt;

&lt;p&gt;2016-07-26 13:39:00.477857 Iteration 16000 loss 3646240.50901&lt;br/&gt;
2016-07-26 13:39:00.477998 Iteration 16000 overall accuracy 0.733180213097
2016-07-26 13:39:00.478118 Iteration 16000 mean accuracy 0.047619047619&lt;br/&gt;
2016-07-26 13:39:00.478241 Iteration 16000 mean IU 0.0349133434808&lt;br/&gt;
2016-07-26 13:39:00.478323 Iteration 16000 fwavacc 0.537553224877&lt;/p&gt;

&lt;p&gt;2016-07-26 17:03:43.480082 Iteration 48000 loss 3076769.60242&lt;br/&gt;
2016-07-26 17:03:43.480186 Iteration 48000 overall accuracy 0.733180213097
2016-07-26 17:03:43.480288 Iteration 48000 mean accuracy 0.047619047619&lt;br/&gt;
2016-07-26 17:03:43.480508 Iteration 48000 mean IU 0.0349133434808&lt;br/&gt;
2016-07-26 17:03:43.480592 Iteration 48000 fwavacc 0.537553224877&lt;br/&gt;
经过近50000次的训练，发现其中loss并没有收敛，而且另外4个指标也完全没有任何改变，和未经过训练的结果相同。可见，这里存在什么问题还没有弄清楚，待续。&lt;/p&gt;
</description>
        <pubDate>Wed, 27 Jul 2016 01:09:59 +0800</pubDate>
        <link>http://yourdomain.com/segmentation/2016/07/27/train-on-pascal.html</link>
        <guid isPermaLink="true">http://yourdomain.com/segmentation/2016/07/27/train-on-pascal.html</guid>
        
        
        <category>segmentation</category>
        
      </item>
    
      <item>
        <title>Train on CamVid</title>
        <description>&lt;p&gt;SegNet on CamVid&lt;/p&gt;

&lt;p&gt;-- SegNet + pretrained VGG16
-&gt; 10000 iteration (currently)
   Global acc = 0.85041 Class average acc = 0.48181 Mean Int over Union = 0.414&lt;/p&gt;
</description>
        <pubDate>Wed, 27 Jul 2016 01:09:59 +0800</pubDate>
        <link>http://yourdomain.com/segmentation/2016/07/27/train-on-camvid.html</link>
        <guid isPermaLink="true">http://yourdomain.com/segmentation/2016/07/27/train-on-camvid.html</guid>
        
        
        <category>segmentation</category>
        
      </item>
    
      <item>
        <title>solver文件各参数说明文档</title>
        <description>&lt;pre&gt;&lt;code&gt;train_net/test_net ： 网络定义文件的路径
test_iter : 如果数据库有1000个样本，batch_size为10，则需要100次iterations. 因而定义了test_iter，则表示测试阶段每次数据量batch_size
test_interval : 训练阶段，定义迭代的次数进行测试，如500次迭代一次测试
base_lr ： 初始学习速率
- fixed : 保持base_lr不变
- step ： 需设置stepsize, 返回base_lr * gamma ^ (floor(iter / stepsize))
- inv : 需设置power，返回base_lr * (1 + gamma*iter)^(-power)
- poly : 进行多项式误差，返回base_lr*(1-iter/max_iter)^power
- sigmoid ： 进行sigmoid衰减，返回base_lr*(1/(1+exp(-gamma*(iter-stepsize))))
momentum : 含该参数的sgd
weight_decay : 权重衰减
lr_policy : 学习参数的衰减方式
gamma, power
&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Wed, 27 Jul 2016 00:44:59 +0800</pubDate>
        <link>http://yourdomain.com/segmentation/2016/07/27/train-overall.html</link>
        <guid isPermaLink="true">http://yourdomain.com/segmentation/2016/07/27/train-overall.html</guid>
        
        
        <category>segmentation</category>
        
      </item>
    
  </channel>
</rss>
